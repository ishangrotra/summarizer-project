{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MzNmrU8ve2F"
      },
      "outputs": [],
      "source": [
        "!pip install accelerate -q\n",
        "!pip install bitsandbytes -q\n",
        "!pip install streamlit -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "@st.cache_resource\n",
        "def get_model():\n",
        "  device = \"cuda\" # the device to load the model onto\n",
        "  token='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'\n",
        "  model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\",use_auth_token=token,device_map=\"auto\",load_in_8bit=True,pad_token_id=0)\n",
        "  tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\",use_auth_token=token)\n",
        "  return model,tokenizer\n",
        "model,tokenizer=get_model()\n",
        "result=st.text_input(\"Enter news to summarize\", value=\"\", max_chars=500)\n",
        "def summarize(result):\n",
        "  messages = [\n",
        "      {\"role\": \"user\", \"content\": \"you will be given the body the title the author name and date of publishing of an article your task it to summarize it in 50 words  also it should be in a positive and professional tone and all the context of the body should be incorporated in the summary and output the summary the title name of the author and date of publication\"+ ' '+ str(result)}\n",
        "  ]\n",
        "  device='cuda'\n",
        "\n",
        "  encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
        "\n",
        "  model_inputs = encodeds.to(device)\n",
        "  generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\n",
        "  decoded = tokenizer.batch_decode(generated_ids)\n",
        "  summ=decoded[0].split('[/INST] ')[-1]\n",
        "  return summ\n",
        "summ=summarize(result[0])\n",
        "st.write(summ)"
      ],
      "metadata": {
        "id": "8HxGoyezv6eF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}