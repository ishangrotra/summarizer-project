{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1oR7lltrWm8"
      },
      "outputs": [],
      "source": [
        "!pip install streamlit -q\n",
        "!pip install langdetect -q\n",
        "!pip install newspaper3k -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O - ipv4.icanhazip.com"
      ],
      "metadata": {
        "id": "XL0PT-LOuBXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "1ZLvM4ItuEK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import newspaper\n",
        "import pandas as pd\n",
        "import requests\n",
        "from langdetect import detect\n",
        "import time\n",
        "from newspaper.article import Article, ArticleException\n",
        "link = st.text_input(\"Enter link to scrape\", value=\"\", max_chars=200)\n",
        "def scrape_article(url, target_date, max_retries=3):\n",
        "    retry_count = 0\n",
        "    article_data = []\n",
        "\n",
        "    while retry_count < max_retries:\n",
        "        try:\n",
        "            article = Article(url, language='en')\n",
        "            article.download()\n",
        "            article.parse()\n",
        "\n",
        "            if len(article.text) < 10:\n",
        "                print(f\"Skipping article with insufficient text: {article.url}\")\n",
        "                break\n",
        "\n",
        "            article_language = detect(article.text)\n",
        "            if article_language == 'en':\n",
        "                if (\n",
        "                    article.publish_date\n",
        "                    and article.publish_date.date() > pd.to_datetime(target_date).date()\n",
        "                ):\n",
        "                    article_data.append({\n",
        "                        'title': article.title,\n",
        "                        'body': article.text,\n",
        "                        'author': article.authors if article.authors else \"No author found\",\n",
        "                        'publish_date': article.publish_date.strftime('%Y-%m-%d')\n",
        "                    })\n",
        "                else:\n",
        "                    print(f\"Skipping older article: {article.url}\")\n",
        "                break\n",
        "            else:\n",
        "                print(f\"Skipping non-English article: {article.url}\")\n",
        "                break\n",
        "        except ArticleException as ae:\n",
        "            print(f\"Failed to process article: {str(ae)}\")\n",
        "            retry_count += 1\n",
        "            time.sleep(2)  # Wait for a short time before retrying\n",
        "            continue\n",
        "        except Exception as e:\n",
        "            print(f\"Unhandled exception: {str(e)}\")\n",
        "            break\n",
        "\n",
        "    if not article_data:\n",
        "        print(\"No article found matching the criteria.\")\n",
        "        return None\n",
        "\n",
        "    return article_data\n",
        "\n",
        "target_date = '2022-01-16'\n",
        "result = scrape_article(link, target_date)\n",
        "\n",
        "if result is not None:\n",
        "    st.write(result)\n",
        "else:\n",
        "    st.write(\"No data to display.\")"
      ],
      "metadata": {
        "id": "YeNdKxSgsREG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}