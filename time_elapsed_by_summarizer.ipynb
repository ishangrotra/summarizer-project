{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_Kk6Rk748kY"
      },
      "outputs": [],
      "source": [
        "!pip install accelerate -q\n",
        "!pip install bitsandbytes -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import time"
      ],
      "metadata": {
        "id": "yXXNEgXc7iof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model():\n",
        "  device = \"cuda\" # the device to load the model onto\n",
        "  token='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'\n",
        "  model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\",use_auth_token=token,device_map=\"auto\",load_in_8bit=True,pad_token_id=0)\n",
        "  tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\",use_auth_token=token)\n",
        "  return model,tokenizer\n",
        "def summarize(result):\n",
        "  messages = [\n",
        "      {\"role\": \"user\", \"content\": \"you will be given the body the title the author name and date of publishing of an article your task it to summarize it in 50 words  also it should be in a positive and professional tone and all the context of the body should be incorporated in the summary and output the summary the title name of the author and date of publication\"+ ' '+ str(result)}\n",
        "  ]\n",
        "  device='cuda'\n",
        "\n",
        "  encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
        "\n",
        "  model_inputs = encodeds.to(device)\n",
        "  generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\n",
        "  decoded = tokenizer.batch_decode(generated_ids)\n",
        "  summ=decoded[0].split('[/INST] ')[-1]\n",
        "  return summ\n"
      ],
      "metadata": {
        "id": "Gtf4BpY17oDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "model,tokenizer=get_model()\n",
        "summ=summarize(summary)#enter summary here\n",
        "print(summ)\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"Time taken: {elapsed_time:.2f} seconds\")"
      ],
      "metadata": {
        "id": "aJhoDZF27vIv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}